{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "afa44044-d2fb-47d8-a588-1e293e235509",
   "metadata": {},
   "source": [
    "# **ASSIGNMENT**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc1f709f-bc5b-4318-bfb0-c050df5767d9",
   "metadata": {},
   "source": [
    "**Q1. What is Lasso Regression, and how does it differ from other regression techniques?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad90206f-008d-4a37-9871-8555f0378a14",
   "metadata": {},
   "source": [
    "Lasso Regression, or L1 regularization, is a linear regression technique that includes a penalty term based on the absolute values of the coefficients. The term \"Lasso\" stands for Least Absolute Shrinkage and Selection Operator. In Lasso Regression, the objective function is the sum of the squared differences between the observed and predicted values (similar to ordinary least squares regression), plus the absolute values of the coefficients multiplied by a regularization parameter (lambda or alpha).\n",
    "\n",
    "The objective function for Lasso Regression can be represented as:\n",
    "\n",
    "Objective = RSS + sum_{i=1}^{n} |\\beta_i|\n",
    "\n",
    "Where:\n",
    "- RSS is the residual sum of squares (sum of squared differences between observed and predicted values).\n",
    "- lambda is the regularization parameter.\n",
    "- beta_i represents the coefficients of the features.\n",
    "\n",
    "The key difference between Lasso Regression and other regression techniques, such as Ridge Regression, is in the penalty term. Lasso uses the absolute values of the coefficients (L1 norm), while Ridge uses the squared values of the coefficients (L2 norm). This fundamental difference has implications for the model's behavior and the resulting feature selection.\n",
    "\n",
    "Differences from Ordinary Least Squares (OLS) Regression:\n",
    "\n",
    "1. **Regularization:** Lasso introduces a regularization term, which helps prevent overfitting by penalizing the model for having too many non-zero coefficients.\n",
    "\n",
    "2. **Feature Selection:** One of the notable features of Lasso Regression is that it tends to produce sparse models, meaning it can drive some of the coefficients to exactly zero. This results in automatic feature selection, making Lasso useful when dealing with high-dimensional datasets with many irrelevant or redundant features.\n",
    "\n",
    "3. **Shrinkage:** The regularization term in Lasso also induces shrinkage of coefficient values. This can be particularly beneficial when dealing with multicollinearity among the features.\n",
    "\n",
    "4. **Solution Path:** The optimization path of Lasso Regression is not smooth, meaning that some coefficients may be exactly zero at certain values of the regularization parameter. This can be advantageous in scenarios where interpretability and a sparse model are desirable.\n",
    "\n",
    "In summary, Lasso Regression is a regression technique that introduces a penalty term based on the absolute values of the coefficients. It differs from other regression techniques, such as Ridge Regression and Ordinary Least Squares, in its regularization approach and its ability to perform feature selection."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf6d1bdf-765b-4b00-9ac0-cfcb70788c5a",
   "metadata": {},
   "source": [
    "**Q2. What is the main advantage of using Lasso Regression in feature selection?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cae444d-2062-4c47-940b-668e3c2fdb7e",
   "metadata": {},
   "source": [
    "The main advantage of using Lasso Regression in feature selection lies in its ability to automatically shrink some of the coefficients to exactly zero. This characteristic leads to a sparse model, where only a subset of the features is included in the final model, effectively performing feature selection. Here are the key advantages of Lasso Regression in the context of feature selection:\n",
    "\n",
    "1. **Automatic Feature Selection:** Lasso's regularization term, which involves the absolute values of the coefficients, tends to drive some of the coefficients to zero during the optimization process. As a result, Lasso automatically selects a subset of features that are deemed most relevant for predicting the target variable. This is particularly useful when dealing with high-dimensional datasets where many features may be irrelevant or redundant.\n",
    "\n",
    "2. **Simplicity and Interpretability:** The sparsity induced by Lasso makes the model simpler and more interpretable. By eliminating irrelevant features, the model becomes more focused on the essential predictors, making it easier for analysts and stakeholders to understand and interpret the model.\n",
    "\n",
    "3. **Addressing Multicollinearity:** Lasso's ability to shrink and eliminate certain coefficients is beneficial in the presence of multicollinearity, where predictor variables are highly correlated. In such cases, Lasso can choose one variable from a group of correlated variables and reduce their collective impact on the model.\n",
    "\n",
    "4. **Improved Generalization:** The feature selection provided by Lasso can lead to improved generalization performance. By excluding irrelevant features, the model is less likely to overfit to noise in the training data, resulting in better performance on new, unseen data.\n",
    "\n",
    "5. **Sparse Solution Path:** Lasso Regression has a solution path that is not smooth, meaning that the coefficients may change abruptly as the regularization parameter varies. This can be advantageous for scenarios where a subset of features is expected to have a significant impact on the target variable.\n",
    "\n",
    "It's important to note that while Lasso Regression is powerful for feature selection, the choice of the regularization parameter (often denoted as \\(\\lambda\\)) is crucial. The value of \\(\\lambda\\) determines the extent of regularization, and cross-validation is commonly used to select an appropriate value that balances model complexity and predictive performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39192397-26e0-4fd4-93da-f1a536d99d9d",
   "metadata": {},
   "source": [
    "**Q3. How do you interpret the coefficients of a Lasso Regression model?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bdb39c9-9b6d-4d16-a80b-9fa5a813277f",
   "metadata": {},
   "source": [
    "Interpreting the coefficients of a Lasso Regression model involves understanding the impact of each feature on the predicted outcome, taking into account the regularization effects of L1 regularization. The coefficients in a Lasso model are influenced by both the traditional linear regression relationship and the penalty term associated with the absolute values of the coefficients. Here are some key points to consider when interpreting the coefficients:\n",
    "\n",
    "1. **Magnitude of Coefficients:**\n",
    "   - A positive coefficient indicates a positive relationship with the target variable, while a negative coefficient indicates a negative relationship.\n",
    "   - The magnitude of the coefficient reflects the strength of the impact of the corresponding feature on the target variable.\n",
    "\n",
    "2. **Zero Coefficients:**\n",
    "   - Due to the L1 regularization term in Lasso, some coefficients may be exactly zero.\n",
    "   - Features with zero coefficients are effectively excluded from the model, implying that these features are not contributing to the prediction.\n",
    "   - The selection of features with non-zero coefficients is part of the feature selection capability of Lasso.\n",
    "\n",
    "3. **Significance of Non-Zero Coefficients:**\n",
    "   - Features with non-zero coefficients are considered significant contributors to the model.\n",
    "   - The sign of the coefficient indicates the direction of the relationship, and the magnitude reflects the strength of the impact on the target variable.\n",
    "\n",
    "4. **Interactions and Dependencies:**\n",
    "   - Consider the coefficients in the context of potential interactions and dependencies among features. The impact of a feature may be influenced by the presence or absence of other features.\n",
    "\n",
    "5. **Regularization Impact:**\n",
    "   - Lasso introduces a penalty term based on the absolute values of the coefficients (\\(\\lambda \\sum_{i=1}^{n} |\\beta_i|\\)).\n",
    "   - The regularization term encourages sparsity, leading to some coefficients being exactly zero. This can be interpreted as automatic feature selection.\n",
    "\n",
    "6. **Cross-Validation and \\(\\lambda\\):**\n",
    "   - The choice of the regularization parameter (\\(\\lambda\\)) is critical in Lasso Regression.\n",
    "   - Cross-validation is often used to select an appropriate \\(\\lambda\\) value, balancing model complexity and predictive performance.\n",
    "\n",
    "It's important to note that interpreting coefficients in any regression model, including Lasso, requires caution. Correlation does not imply causation, and the coefficients only represent associations. Additionally, the interpretation may vary based on the scale and nature of the features. Regularization techniques like Lasso are particularly useful for preventing overfitting and identifying a parsimonious set of features but may require careful consideration of the specific context in which the model is applied."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cab3c99c-4da1-44bd-9f63-c4eaf1abc5de",
   "metadata": {},
   "source": [
    "**Q4. What are the tuning parameters that can be adjusted in Lasso Regression, and how do they affect the\n",
    "model's performance?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24d1b147-1aea-4835-b214-58d686de7e53",
   "metadata": {},
   "source": [
    "In Lasso Regression, the main tuning parameter that can be adjusted is often denoted as \\(\\lambda\\) (lambda), which controls the strength of the regularization. The regularization term in Lasso is proportional to the absolute values of the coefficients (\\(\\lambda \\sum_{i=1}^{n} |\\beta_i|\\)), and adjusting \\(\\lambda\\) influences the trade-off between fitting the data well and keeping the model simple. The larger the \\(\\lambda\\), the stronger the regularization, and more coefficients are driven towards zero.\n",
    "\n",
    "Here are the key aspects of tuning the \\(\\lambda\\) parameter in Lasso Regression and how it affects the model's performance:\n",
    "\n",
    "1. **\\(\\lambda\\) Parameter:**\n",
    "   - \\(\\lambda\\) is a positive parameter that determines the strength of the penalty term in the Lasso objective function.\n",
    "   - A small \\(\\lambda\\) results in less regularization, allowing more coefficients to take non-zero values.\n",
    "   - A large \\(\\lambda\\) increases the regularization strength, encouraging sparsity in the model by driving more coefficients to exactly zero.\n",
    "\n",
    "2. **Impact on Sparsity:**\n",
    "   - As \\(\\lambda\\) increases, more coefficients in the Lasso model tend to become exactly zero.\n",
    "   - High \\(\\lambda\\) values promote feature selection, as less important or redundant features are effectively excluded from the model.\n",
    "\n",
    "3. **Bias-Variance Trade-Off:**\n",
    "   - Adjusting \\(\\lambda\\) involves a bias-variance trade-off. Small \\(\\lambda\\) values lead to lower bias but higher variance, potentially resulting in overfitting.\n",
    "   - Large \\(\\lambda\\) values increase bias but reduce variance, helping to prevent overfitting and improve generalization to new data.\n",
    "\n",
    "4. **Cross-Validation:**\n",
    "   - Cross-validation is commonly used to choose an optimal \\(\\lambda\\) value.\n",
    "   - By training the Lasso model with different \\(\\lambda\\) values on subsets of the data and evaluating performance on validation sets, one can identify the \\(\\lambda\\) that provides the best balance between model complexity and predictive accuracy.\n",
    "\n",
    "5. **Regularization Path:**\n",
    "   - The regularization path of Lasso shows how the coefficients change as \\(\\lambda\\) varies.\n",
    "   - It is common to visualize the regularization path to understand the impact of \\(\\lambda\\) on the sparsity of the model and the magnitude of the coefficients.\n",
    "\n",
    "6. **Sensitivity to Scaling:**\n",
    "   - Lasso is sensitive to the scale of the features. It is often recommended to standardize or normalize the features before applying Lasso to ensure that all features are on a similar scale.\n",
    "\n",
    "In summary, adjusting the \\(\\lambda\\) parameter in Lasso Regression is crucial for controlling the balance between model complexity and regularization strength. The appropriate choice of \\(\\lambda\\) depends on the specific characteristics of the dataset and the modeling goals, and cross-validation is a valuable tool for making this determination."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e1f57bb-1054-4d7e-8338-b8b6aa105725",
   "metadata": {},
   "source": [
    "**Q5. Can Lasso Regression be used for non-linear regression problems? If yes, how?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5de54478-6240-4344-9e24-69a54f2f7303",
   "metadata": {},
   "source": [
    "Lasso Regression, in its standard form, is a linear regression technique that assumes a linear relationship between the features and the target variable. Therefore, it is inherently designed for linear regression problems. However, it is possible to extend the idea of Lasso to address non-linear regression problems through the use of non-linear transformations of the features.\n",
    "\n",
    "Here are a few approaches to apply Lasso Regression to non-linear regression problems:\n",
    "\n",
    "1. **Feature Engineering:**\n",
    "   - Introduce non-linear transformations of the features to capture non-linear relationships. For example, you can include squared terms, cubic terms, or other polynomial features.\n",
    "   - Suppose you have a feature \\(X\\), you can create new features like \\(X^2\\), \\(X^3\\), etc. This allows the model to capture quadratic, cubic, or higher-order relationships.\n",
    "\n",
    "2. **Basis Expansion:**\n",
    "   - Use basis expansion techniques to transform features into a higher-dimensional space. This can involve using basis functions such as polynomial basis functions or radial basis functions (RBFs).\n",
    "   - For instance, instead of using \\(X\\), you might use basis functions like \\(\\phi(X) = [1, X, X^2, \\ldots]\\) or radial basis functions based on the distance from certain points.\n",
    "\n",
    "3. **Kernelized Lasso:**\n",
    "   - Apply the concept of kernel methods to Lasso. Kernelized Lasso involves transforming the input features into a higher-dimensional space using a kernel function, and then applying Lasso in that space.\n",
    "   - Common kernels include polynomial kernels and radial basis function kernels.\n",
    "\n",
    "It's important to note that while these approaches allow Lasso Regression to capture non-linear relationships, they also introduce the challenge of selecting appropriate hyperparameters, such as the degree of polynomial features or the choice of kernel. Additionally, increasing the complexity of the model can lead to overfitting, so careful validation and regularization are essential.\n",
    "\n",
    "If the non-linearities in the data are complex and cannot be adequately captured by simple polynomial or basis expansions, more sophisticated non-linear regression techniques, such as support vector machines, decision trees, or neural networks, might be considered as alternatives. These models are inherently capable of capturing complex non-linear relationships without the need for explicit feature engineering."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cb6d1d2-c301-4677-b382-9346d32205d8",
   "metadata": {},
   "source": [
    "**Q6. What is the difference between Ridge Regression and Lasso Regression?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ef160d1-99f2-44a2-948f-5bccb4fd3aa7",
   "metadata": {},
   "source": [
    "Ridge Regression and Lasso Regression are both regularization techniques used in linear regression to address the problem of multicollinearity and overfitting, but they differ in the type of regularization they apply. Here are the key differences between Ridge Regression and Lasso Regression:\n",
    "\n",
    "1. **Regularization Term:**\n",
    "   - **Ridge Regression:** Adds a regularization term to the linear regression objective function based on the sum of squared values of the coefficients. The regularization term is proportional to \\(\\lambda \\sum_{i=1}^{n} \\beta_i^2\\), where \\(\\lambda\\) is the regularization parameter and \\(\\beta_i\\) represents the coefficients.\n",
    "   - **Lasso Regression:** Adds a regularization term based on the sum of the absolute values of the coefficients. The regularization term is proportional to \\(\\lambda \\sum_{i=1}^{n} |\\beta_i|\\).\n",
    "\n",
    "2. **Type of Regularization:**\n",
    "   - **Ridge Regression:** L2 regularization, also known as Tikhonov regularization or L2 norm regularization, penalizes the sum of squared coefficients. It does not usually lead to exact zero coefficients, but it shrinks them towards zero.\n",
    "   - **Lasso Regression:** L1 regularization, or L1 norm regularization, penalizes the sum of the absolute values of the coefficients. It has a tendency to drive some coefficients exactly to zero, leading to sparsity in the model.\n",
    "\n",
    "3. **Impact on Coefficients:**\n",
    "   - **Ridge Regression:** All coefficients are shrunk towards zero, but none are usually exactly zero. Ridge tends to work well when there is multicollinearity among the features.\n",
    "   - **Lasso Regression:** Some coefficients are exactly zero, resulting in automatic feature selection. Lasso is particularly useful when feature selection is desired or when dealing with high-dimensional datasets with many irrelevant features.\n",
    "\n",
    "4. **Solution Path:**\n",
    "   - **Ridge Regression:** The regularization path of Ridge Regression is smooth, and the coefficients decrease gradually as \\(\\lambda\\) increases.\n",
    "   - **Lasso Regression:** The regularization path of Lasso Regression is not smooth. As \\(\\lambda\\) increases, some coefficients may suddenly become zero, leading to a sparse solution.\n",
    "\n",
    "5. **Number of Selected Features:**\n",
    "   - **Ridge Regression:** Retains all features but shrinks their coefficients.\n",
    "   - **Lasso Regression:** Can lead to a sparse model with only a subset of features having non-zero coefficients.\n",
    "\n",
    "6. **Sensitivity to Outliers:**\n",
    "   - **Ridge Regression:** Less sensitive to outliers compared to Lasso.\n",
    "   - **Lasso Regression:** Can be sensitive to outliers, and the presence of outliers may have a larger impact on feature selection.\n",
    "\n",
    "In summary, the main difference between Ridge and Lasso lies in the type of regularization they apply and the impact on the coefficients. Ridge uses L2 regularization and shrinks coefficients, while Lasso uses L1 regularization, leading to sparsity and potential feature selection. The choice between Ridge and Lasso depends on the specific characteristics of the data and the modeling goals. In some cases, a combination of both, known as Elastic Net, is used to benefit from both L1 and L2 regularization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21c45faa-00b9-4fea-96d8-ab1dd0890bf4",
   "metadata": {},
   "source": [
    "**Q7. Can Lasso Regression handle multicollinearity in the input features? If yes, how?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d366581d-c60f-470a-9ade-712af0b24942",
   "metadata": {},
   "source": [
    "Yes, Lasso Regression can handle multicollinearity in the input features to some extent. Multicollinearity occurs when two or more features in a regression model are highly correlated, which can lead to numerical instability and inflated standard errors of the estimated coefficients. Lasso Regression, with its L1 regularization term, has a particular property that helps address multicollinearity:\n",
    "\n",
    "1. **Automatic Feature Selection:**\n",
    "   - One of the key features of Lasso Regression is its ability to perform automatic feature selection by driving some coefficients exactly to zero.\n",
    "   - When multicollinearity is present, Lasso tends to select one feature from a group of highly correlated features and sets the coefficients of the less important features to zero.\n",
    "   - This automatic feature selection can help mitigate the effects of multicollinearity by excluding redundant or less informative features from the model.\n",
    "\n",
    "2. **Sparse Solutions:**\n",
    "   - The L1 regularization term in Lasso introduces sparsity in the model, meaning that only a subset of features will have non-zero coefficients in the final solution.\n",
    "   - By promoting sparsity, Lasso naturally deals with multicollinearity by effectively ignoring some of the correlated features.\n",
    "\n",
    "3. **Shrinkage Effect:**\n",
    "   - The shrinkage effect induced by Lasso can help reduce the impact of multicollinearity on the estimated coefficients.\n",
    "   - The regularization term penalizes the sum of the absolute values of the coefficients, and as a result, it tends to shrink and dampen the influence of correlated features.\n",
    "\n",
    "While Lasso Regression can be effective in handling multicollinearity, it's important to note a few considerations:\n",
    "\n",
    "- **Feature Selection Trade-Off:** While Lasso can handle multicollinearity, it may also lead to the exclusion of potentially relevant features. The choice of the regularization parameter (\\(\\lambda\\)) plays a crucial role in balancing feature selection and model fit.\n",
    "\n",
    "- **Sensitivity to Scaling:** Lasso is sensitive to the scale of the features. It's often recommended to standardize or normalize the features before applying Lasso to ensure that all features are on a similar scale.\n",
    "\n",
    "- **Interaction Effects:** Lasso may have limitations in capturing interaction effects between correlated features. In some cases, incorporating interaction terms explicitly or using other modeling techniques might be necessary.\n",
    "\n",
    "In summary, Lasso Regression's ability to perform feature selection by driving some coefficients to zero makes it a useful tool for handling multicollinearity. However, the choice of hyperparameters and careful consideration of the specific context are essential for achieving optimal results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b957422b-68e1-434d-9870-bc0522a9ef35",
   "metadata": {},
   "source": [
    "**Q8. How do you choose the optimal value of the regularization parameter (lambda) in Lasso Regression?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad1c9100-f68b-4957-a44c-bf05fd336445",
   "metadata": {},
   "source": [
    "Choosing the optimal value of the regularization parameter (\\(\\lambda\\)) in Lasso Regression is a crucial step, and cross-validation is a common approach to make this selection. The goal is to find the \\(\\lambda\\) that provides the best balance between model simplicity (sparsity) and predictive performance. Here's a step-by-step process for choosing the optimal \\(\\lambda\\):\n",
    "\n",
    "1. **Define a Range of \\(\\lambda\\) Values:**\n",
    "   - Start by defining a range of \\(\\lambda\\) values to explore. This range typically spans several orders of magnitude, covering both very small and large values.\n",
    "   - Commonly used values are in the form of \\(\\lambda = 10^k\\), where \\(k\\) varies.\n",
    "\n",
    "2. **Cross-Validation:**\n",
    "   - Use \\(k\\)-fold cross-validation to evaluate the performance of the Lasso Regression model for each \\(\\lambda\\) value.\n",
    "   - The dataset is divided into \\(k\\) folds, and the model is trained and validated \\(k\\) times, each time using a different fold for validation and the remaining folds for training.\n",
    "\n",
    "3. **Performance Metric:**\n",
    "   - Choose a performance metric to evaluate the model's performance during cross-validation. Common metrics include mean squared error (MSE), mean absolute error (MAE), or \\(R^2\\) (coefficient of determination).\n",
    "   - The goal is to select the \\(\\lambda\\) that minimizes the chosen performance metric.\n",
    "\n",
    "4. **Select Optimal \\(\\lambda\\):**\n",
    "   - Identify the \\(\\lambda\\) value that results in the best performance on the validation set. This is often referred to as the optimal \\(\\lambda\\) or the \\(\\lambda\\) that minimizes the validation error.\n",
    "\n",
    "5. **Refinement:**\n",
    "   - If needed, perform a more refined search around the identified optimal \\(\\lambda\\) value to ensure that the selected value is indeed the best.\n",
    "   - This can involve using a smaller range of \\(\\lambda\\) values and performing a more granular search.\n",
    "\n",
    "6. **Final Model:**\n",
    "   - Train the final Lasso Regression model using the selected optimal \\(\\lambda\\) on the entire dataset (training and validation sets combined).\n",
    "\n",
    "7. **Evaluate on Test Set:**\n",
    "   - Evaluate the final model on a separate test set to assess its generalization performance.\n",
    "\n",
    "Common techniques for implementing this process include grid search, randomized search, or more advanced optimization methods. Libraries like scikit-learn in Python provide convenient functions for performing cross-validated grid search.\n",
    "\n",
    "In Python, using scikit-learn, the process might look like this:\n",
    "\n",
    "```python\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Create a Lasso Regression model\n",
    "lasso_model = Lasso()\n",
    "\n",
    "# Define a range of lambda values\n",
    "param_grid = {'alpha': [0.001, 0.01, 0.1, 1, 10, 100]}\n",
    "\n",
    "# Perform grid search with cross-validation\n",
    "grid_search = GridSearchCV(lasso_model, param_grid, cv=5, scoring='neg_mean_squared_error')\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best lambda value\n",
    "best_lambda = grid_search.best_params_['alpha']\n",
    "\n",
    "# Train the final model with the best lambda value\n",
    "final_model = Lasso(alpha=best_lambda)\n",
    "final_model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate on the test set\n",
    "test_predictions = final_model.predict(X_test)\n",
    "```\n",
    "\n",
    "This code snippet demonstrates how to perform a grid search over a range of \\(\\lambda\\) values using cross-validation and select the best \\(\\lambda\\) based on the mean squared error. The final model is then trained with the chosen \\(\\lambda\\) and evaluated on a separate test set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a091d728-8329-46d6-a66e-b0d1bdddfce3",
   "metadata": {},
   "source": [
    "-----------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd63f965-7c05-4a02-ac8f-af466879b611",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
